{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create --name rag-project python=3.12\n",
    "#conda activate rag-project\n",
    "#pip install langchain langchain-text-splitters langchain-community bs4\n",
    "#pip install \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24482286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import getpass\n",
    "import os\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from typing import Any\n",
    "from langchain.messages import RemoveMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993e6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(f\"Langchain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64036f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb743cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "load_dotenv() # Load environment variables from .env file\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "#model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cb02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a3bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d35ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f0d909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b41e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f41326f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8470542f-d568-45b9-afc3-c170cc220737', '6b5b3cc2-85ef-4ad8-a9d5-1b98467e502e', 'f0e8ef92-412e-4d34-b9db-67883f7e59ec']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2070b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the vector store to disk\n",
    "vector_store.dump('vectorr_store_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac4d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embedded vectors\n",
    "vector_store = InMemoryVectorStore.load('vectorr_store_original', embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a317e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63586f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467921e4",
   "metadata": {},
   "source": [
    "## Memory concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a375ca",
   "metadata": {},
   "source": [
    "### Stored Chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74216b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"When you receive a response from a tool, you MUST summarize it and provide a final answer to the user. \"\n",
    "    \"DO NOT return an empty response. \"\n",
    "    \"Always synthesize the information retrieved.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt, checkpointer=InMemorySaver(), middleware=[trim_messages], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8d54a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='03292f73-9f72-4a98-a7d6-ba496cc2c90c'),\n",
       "  AIMessage(content='Hi Bob! How can I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b269a-1f2a-7f70-808a-6751a9a1bd11-0', usage_metadata={'input_tokens': 89, 'output_tokens': 10, 'total_tokens': 99, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='781012e8-26f4-4c6f-884d-3bbce4404e3e'),\n",
       "  AIMessage(content=\"Hi Bob! It's nice to meet you. I'm a large language model, and I'm here to help with any questions or tasks you might have.\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b269b-83c7-7643-a2f5-0d7bb54a8ceb-0', usage_metadata={'input_tokens': 107, 'output_tokens': 35, 'total_tokens': 142, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='6d4e3878-1a4a-4858-b2b7-cbeb8b53d808'),\n",
       "  AIMessage(content='Soft paws tread, a silent grace,\\nEyes of emerald, in this place.\\nA gentle purr, a rumbling sound,\\nAs feline friends curl and surround.\\nWith twitching tail and whiskers keen,\\nA cozy nap, a peaceful scene.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b269b-889c-72e2-b5a4-70393487aaf8-0', usage_metadata={'input_tokens': 150, 'output_tokens': 54, 'total_tokens': 204, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='6c970c27-3181-4878-90e3-837978217cab'),\n",
       "  AIMessage(content=\"A wagging tail, a happy bark,\\nLoyal companion, from dawn till dark.\\nWith sloppy kisses and a joyful leap,\\nSecrets of the heart, they softly keep.\\nThrough fields they run, with boundless glee,\\nMan's best friend, wild and free.\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b269b-8d13-7910-9fd0-ca01e57e7696-0', usage_metadata={'input_tokens': 213, 'output_tokens': 59, 'total_tokens': 272, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c12ad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d5a53",
   "metadata": {},
   "source": [
    "### Summary concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=(\"tokens\", 4000),\n",
    "            keep=(\"messages\", 20)\n",
    "        )\n",
    "    ],\n",
    "    checkpointer=checkpointer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa88464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e05fa",
   "metadata": {},
   "source": [
    "## Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import after_model\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "\n",
    "@after_model\n",
    "def validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n",
    "    \"\"\"Remove messages containing sensitive words.\"\"\"\n",
    "    STOP_WORDS = [\"password\", \"secret\"]\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if any(word in last_message.content for word in STOP_WORDS):\n",
    "        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[],\n",
    "    middleware=[validate_response],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
